% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{intmacros}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{Assignment 1: EM for Categorical Data Advanced Signal Processing}%
\author{Daniel Barrej√≥n Moreno} %if necessary, replace with your course title
 
\maketitle
 
\section*{Formulation of the problem}
 
\textbf{ Mixture model of categorical} \begin{equation}
     p(\x|\bstheta) = \sum_{k=1}^{K}\pi_k\prod_{j=1}^{D}\Cat(x_j|\bstheta_k)
 \end{equation}

\textbf{Incomplete data likelihood}
\begin{equation}
    p(\X,\bstheta) = \prod_{i=1}^{N}\sum_{k=1}^{K}\pi_k\prod_{j=1}^{D}\Cat(x_{ij}|\bstheta)
\end{equation}

\textbf{PDF x given z}
\begin{equation}
p(\x_i|z_i,\bstheta) = \prod_{k=1}^{K}\prod_{j=1}^{D}\Cat(x_{ij}|\bstheta_k)^{\Ind (z_i=k)}
\end{equation}

\textbf{Marginal for z}
\begin{equation}
    p(z_i|\bstheta) = \prod_{k=1}^{K}\pi_k^{\Ind (z_i=k)}
\end{equation}

\textbf{Marginal for x}
\begin{align}
    p(\x_i) & = \prod_{j=1}^D\Cat(x_{ij}|\bstheta) = \prod_{j=1}^D\prod_{m=1}^I\theta_m^{\Ind(x_{i,j}=m)} = \prod_{m=1}^I\prod_{j=1}^D\theta_m^{\Ind(x_{i,j}=m)} \\
    & = \prod_{m=1}^I\theta_m^{\sum_{j=1}^D {\Ind(x_{i,j}=m)}} = \prod_{m=1}^I \theta_m^{\mu_{i,m}},
\end{align}
where
\begin{equation}
    \mu_{i,m} = \sum_{j=1}^D {\Ind(x_{i,j}=m)}
\end{equation}

\section{Log-likelihood $l_c$}
\textbf{Complete data log-likelihood}
\begin{align}
    l_c(\bstheta) & = \ln p(\mathcal{D},\mathcal{Z}|\bstheta) = \sum_{i=1}^{N}\log (p(\x_i|\z_i,\bstheta) p(\z_i|\bstheta)) =  \sum_{i=1}^{N}\log \left(\prod_{k=1}^K(\pi_k\prod_{j=1}^D \Cat(x_{ij}|\bstheta_k))^{\Ind (z_i=k)}\right)\\
    & = \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \log\left( \pi_k\prod_{j=1}^D\Cat(x_{ij}|\bstheta_k)\right) \\
    & =  \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \log \pi_k + \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \log \prod_{m=1}^I \theta_m^{\mu_{i,m}}\\
    & = \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \log \pi_k + \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \sum_{m=1}^I \mu_{i,m}\log \theta_m
    \end{align}
    
   
\section{3. E-step: Q function}
\textbf{Expectation Step: }\\

Taking the expectation with respect to the latent variables $z$ we get the following expected complete data log-likelihood.

\begin{align}
    Q(\bstheta^t, \bstheta^{t-1}) & = \expectation_Z\{l_c(\bstheta|\mathcal{D},\bstheta^{t-1})\} = \expectation_Z\{\sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \log \pi_k + \sum_{i=1}^N\sum_{k=1}^K\Ind (z_i=k) \sum_{m=1}^I \mu_{i,m}\log \theta_m\}\\
    & = \sum_{i=1}^N\sum_{k=1}^K\expectation_Z\{\Ind (z_i=k)\} \log \pi_k + \sum_{i=1}^N\sum_{k=1}^K\expectation_Z\{\Ind(z_i=k)\} \sum_{m=1}^I \mu_{i,m}\log \theta_m\\
    & = \sum_{i=1}^N\sum_{k=1}^K r_{i,k}\log \pi_k + \sum_{i=1}^N\sum_{k=1}^K r_{i,k}\sum_{m=1}^I \mu_{i,m}\log \theta_m ,
\end{align}
where 
\begin{align}
	r_{i,k} &= \expectation_Z\{ \Ind(z_i=k) \} = p(z_i = k|\x_i,\bstheta^{t-1}) = \frac{p(z_i=k,\x_i|\bstheta^{t-1})}{p(\x_i|\bstheta^{t-1})} = \frac{p(z_i=k,\x_i|\bstheta^{t-1})}{\sum_{k\prime}^{K}p(z_i=k\prime,\x_i|\bstheta^{t-1})}\\
	& = \frac{\pi_k p(x_i|\bstheta_k)}{\sum_{k\prime}^K \pi_{k\prime} p(x_i|\bstheta_{k\prime})} =
	\frac{\pi_k\prod_{j=1}^{D}\Cat(x_{i,j}|\bstheta_k)}{\sum_{k\prime}^{K} \pi_{k\prime} \prod_{j=1}^{D}\Cat(x_{i,j}|\bstheta_k}
\end{align}


\section{M-step: Maximization}

\subsection{Maximization of $\pi_k$}


\begin{equation}
	\hat{\pi_k} = \frac{\sum_{i=1}^N r_{i,k}}{N} = \frac{N_k}{N},
\end{equation}
where
\begin{equation}
	N_k = \sum_{i=1}^N r_{i,k}
\end{equation}

\subsection{Maximization of $\bstheta_k$}

\begin{equation}
	\hat{\bstheta}_{k,m} = \frac{\sum_{i=1}^N r_{i,k}\mu_{i,m}}{\sum_{m=1}^I \sum_{i=1}^N r_{i,k} \mu_{i,m}}
\end{equation}




\end{document}

